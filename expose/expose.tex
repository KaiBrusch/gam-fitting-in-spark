\documentclass{article}

% Latex Tutorial beginners: http://www.latex-tutorial.com/tutorials/beginners/
\newcommand{\comment}[1]{}

\newcommand{\note}[1]{{\tiny (#1)}}

% Packages explained - Adding more functions to LATEX http://www.latex-tutorial.com/tutorials/beginners/lesson-3/
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{textgreek}

\usepackage[margin=1.3in]{geometry}

% fügt die Literatur auch zum Inhaltsverzeichnis hinzu (ist so nicht default)
\usepackage[nottoc,notlot,notlof,numbib]{tocbibind}

% Adding a bibliography http://www.latex-tutorial.com/tutorials/beginners/lesson-7/
\bibliographystyle{apalike}


% Nested numbered enumerations, eg. 1.1, 1.2: http://tex.stackexchange.com/questions/78842/nested-enumeration-numbering
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}


\title{
    Fitting Generalized Additive Models for\\ very large datasets with Apache Spark \\[7pt]
    \large An exposé for a bachelor thesis
}
\date{02.11.2015}
\author{Kai Thomas Brusch}


\begin{document}

    \pagenumbering{arabic}

    \maketitle

    \paragraph{Summary}

    This exposé describes content, goals and motivation and gives a general introduction to my bachelor thesis titled "Fitting General Additive Models for very large datasets with Apache Spark".

    \paragraph{Contact} \texttt{kai.brusch@gmail.com}

    \paragraph{Location} Hochschule für Angewandte Wissenschaften Hamburg
    \paragraph{Department} Dept. Informatik
    \paragraph{Examiner} Prof. Dr. Michael Köhler-Bußmeier
    \paragraph{Second examiner} TBA

    \newpage

    \tableofcontents

    \newpage

    \section{Introduction}

    \subsection{Context}
    Regression analysis stands at the heart of modern statistical analysis. Discovering the relationship between an dependant and independant variables may offer great insight into the underlying system and can even provide far reaching predictive capabilities. The traditional linear models explain the dependant variable as the sum of the independant variables and the linear interaction with a coefficent.\cite{gamBook} describes the generallized additve model (GAM) as powerful extension of the generalized linear model, explaining the relationship between dependant and independant variables as the sum of smoothing functions. This modeling approach offers more flexiblity and power than linear model and has been succesfuly established in academia and industry. However, the additional power of GAMs comes at the cost of additional memory requirements and computational effort, placing strict constraints on the model and data size.\cite{bigdataGAM} suggest two optimization to eliviate this constraint: parallelization and numerical optimization.
    These optimizations have been implemented and used in high-level statiscial languanges such as R, SPSS and Stata. While these implementations already allow to fit larger models with more data they have never been implemented in a truly distributed enviroment.\cite{spark} introduced Apache Spark which has recently established itself as one of the most promising general purpose cluster computing engines. It's general and distributed nature offers the ideal enviroment to implement the by \cite{bigdataGAM} suggested optimizations to bring GAMs beyond the limitations of high-level statistical languages.



    \section{Content}
    \subsection{General Linear Models}
    Linear models are statistical models in which a univariate response is modelled as the sum of a ‘linear predictor’ and a zero mean random error term. The linear predictor depends on some predictor variables, measured with the response variable, and some unknown parameters, which must be estimated. A key feature of linear models is that the linear predictor depends linearly on these parameters. Statistical inference with such models is usually based on the assumption that the response variable has a normal distribution. Linear models are used widely in most branches of science, both in the analysis of designed experiments. \

    \begin{equation}  \label{linModel} y\textsubscript{i} = \beta x\textsubscript{i} + \varepsilon\textsubscript{i} \end{equation}

    Equation \ref{linModel} introduces formal notation on how to describe the dependant variable y as linear combination of x and the unknown parameter $\beta$ plus an error term \textepsilon. We want $\beta$ to fit all data as closely as possible and hence introduce S as the squared difference between an estimated $x_i \beta$ and $y_i$

    \begin{equation} \label{sumSquares} \mathbf{S} =  \sum_{i=1}^{n}(y\textsubscript{i} - x\textsubscript{i}\beta)^{2} \end{equation}

    A good choice of $\beta$ should bring S close to 0. The  Markov-Gauss Theorem states that the minimization of S in yields $\widehat{\beta}$ which is the best possible estimation of $b$. \ref{linModel} represents the univivariate case where y is explained with only one variable. This model can be extendend to multiple explanatory variables yielding in a scalar form of \ref{linModel}

    \begin{equation}  \label{linMatrixModel} y =  \mathbf{X} \beta \end{equation}

     The dependante variable vector \textit{y} is the linear combination of model matrix \textit{X} and the vector of unknown parameter \textbeta.

    \subsection{Generalized Linear Models}
    Generalized linear models (GLMs) somewhat relax the strict linearity assumption of linear models, by allowing the expected value of the response to depend on a smooth monotonic function of the linear predictor. Similarly the assumption that the response is normally distributed is relaxed by allowing it to follow any distribution from the exponential family (for example, normal, Poisson, binomial, gamma etc.).

    \begin{equation} \label{glmMatrix} g(\mu\textsubscript{i}) = \mathbf{X} \beta \end{equation}

    Equation \ref{glmMatrix} introduces formal notation for GLMs and illustrates the similarities to the genral linear model. It is important to recognize the smooth monotonic function \textit{g()}. The smooth monotonic function, also known as link function, is the key extension which enables GLMs to model members of the expotential family with a linear model. Finding the best possible estimator $\widehat{\beta}$ for \ref{glmMatrix} is formalized stated as the minimal length of the eucledean distance:

    \begin{equation} \label{matrixLOSS} \left \| y - X\beta  \right \|^2 \end{equation}

    Minimizing \ref{matrixLOSS} will yield in the best possible estimator $\widehat{\beta}$. This has been well established and many of the following problems leverage knowledge on how to obtain $\widehat{\beta}$ through \ref{matrixLOSS}.

    \subsection{Generalized Additive Models}
    Generalized Additive Models (GAMs) extends the GLM by specifing the linear prediction in terms of the summation of smooth functions. This allows for a more flexible modelling of the influence for each explanatory variable. The gained flexibility comes at the cost of additional questions:

    \begin{itemize}
        \item The smooth functions must be represented somehow.
        \item The degree of smoothness of the functions must be made controllable.
        \item Some means for estimating the most appropriate degree of smoothness from data is required.
    \end{itemize}

    With the outlined additions to GLMs we can now introduce formal notation for GAMs:

    \begin{equation} \label{GAM} g(\mu\textsubscript{i}) = \mathbf{X} \textsubscript{i} \Theta + f\textsubscript{1}(x\textsubscript{1i}) + f\textsubscript{2}(x\textsubscript{2i}) + f\textsubscript{3}(x\textsubscript{3i}, x\textsubscript{4i}) ... \end{equation}

    \ref{GAM} explains $y_i$ as the model matrix for this row and the smooth functions $f_j(x1j)$ of the x values for this row. $X_i$ is a row of the model matrix with parametric component. $\theta$. Unlike the linear model we can now  specify a smooth function for each explanatory variable. This proves to be way more flexible than only allowing for a constant influece per explanatory variable.  The natural question that arises now is: How do I find a proper smoothing function? Finding the right smooth function stands at the heart of GAM fitting and can be best illustrated in the univariate case \ref{univariateSmooth}.


    \begin{equation} \label{univariateSmooth} y_i = f(x_i) + \epsilon_i \end{equation}

    Literature suggest that the unknown smooth function $f()$ can be best fund by specifiying the a linear base $b()$ for the smooth function. Only allowing linear basis allow us to heavily leverage the theory already developed for linear models and $S$ as the optimal model fit. For the sake of illustration we assume that \ref{univariateSmooth}can be rewrite as the following equation if $b_i(x)$ is the $ith$ basis function:

    \begin{equation} \label{smoothBase} f(x) = \sum_{i=1}^{q} b\textsubscript{i} (x) \beta \textsubscript{i} \end{equation}

    In \ref{smoothBase} we already know $bi$ and that linearity \ref{smoothBase}is linear. We now have to specify a basis function to represent $bi$. We can choose from many basis functions for $bi$, each with advantages and disavantages. A common choice however is a fourth order polynomial allowing basis function. \ref{smoothBase} represented by a fourth order polynomial yields the following model:

    \begin{equation} \label{poly4th} f(x) = \beta \textsubscript{1} + x\beta \textsubscript{2} + x^2\beta \textsubscript{3} + x^3\beta \textsubscript{4} + x^4\beta \textsubscript{5}  \end{equation}

    Applying \ref{poly4th} to \ref{univariateSmooth} we get the description of $yi$ as the sum of smoothing functions.

    \begin{equation} \label{poly4thGAM}  y_i = \beta_1 + x_i\beta_2 + x_i^2\beta_3+ x_i^3\beta_4 + x_i^4\beta_5 + \epsilon_i \end{equation}

    Equation \ref{poly4thGAM} gives a formal full description of  y\textsubscript{i} as the sum of a fourth degree polynomial. Fourth order polynomial basis function enforce rather strict limitations and often struggle to provide the desireable fit. Instead of trying to fit a single fourth order polynomial to all our data points we will now try to represent $f$ with cubic splines. This approach is called regression splines and follows this general pattern:

    \begin{itemize}

    \item Divide the curve in to a fixed number of sections, the point between two sections is called a knot.
    \item Find a cubic polynomial for each section between two knot
    \item Each cubic polynomial must match its neighboors in value, first and second derivative at the location of the knot.
    \item Each spline has zero second derivatives at the knot location to ensure a smooth transition.

    \end{itemize}

    Using cubic splines as a basis for f means that \ref{univariateSmooth} becomes a linear model identical to \ref{linMatrixModel}. We can hence rely well established linear model theory to fit the very flexible class of GAMs.

    The number and location of knots play an essential role in GAM fitting and are commonly defined and hence not relevant. Since they are specified we will not futher eleborate on them.

    After established the basis function and knots we will now turn to the smoothing parameter $\lambda$. $\lambda$ is a penalty factor that controlls the smoothness by scaling the second derivative of the smooth function, this can be formalized by:

    \begin{equation} \label{losswithpenalty} \left \| y - X\beta  \right \|^2 + \lambda \int_{0}^{1} [{f}''(x)]^2 dx \end{equation}

    The integrated square root of second deriviative penalizes models that have a too 'rough' function. As \textlambda approaches 0 $f$ will become a straight regression line while \textlambda of 0 becomes an unpenelized regression spline. The trade off between model fit and and model smoothness is controlled by the smoothing parameter \textlambda.

    The linear nature of $f$ allows us to rewrite \ref{losswithpenalty} in it's final form:

    \begin{equation} \label{FINALSMOOTH} \left \| y - X\beta  \right \|^2 + \lambda \beta^T S\beta  \end{equation}

    \ref{FINALSMOOTH} shows that describing an unknown variable as the sum of smooth functions can be restated as the minimization of penelized regression splines and hence becomes problem of minimizing equation \ref{FINALSMOOTH}. While the the problem of finding a smooth function for a univariate function has almost been solved by \ref{FINALSMOOTH} we are still left with an unknown parameter of \textlambda. Finding the optimal $\textlambda$ is subject of the following section.

    \subsection{Smoothing parameter Estimation with generalized cross validation}
    Finding a good smoothing parameter through minimizing \ref{FINALSMOOTH} poses a central question for GAM research. Finding a optimal choice for \textlambda means to trade goodness of fit to smoothness. A high \textlambda will overfit the data while a low \textlambda will create undersmoothed splines. The common approach is to define a measure for the predictive capabilities of our estimated spline. For our purposes we will use the generalized cross validation (GCV) which formalizes the notion that an ideal splines minimizes the average distance between our splines and the function f in relation to the mean weight: $tr(I-A)/n$ yielding

    \begin{equation} \label{FULLGCV} V_g(\lambda) = \frac{n\left \| y-X\widehat{\beta}_\lambda \right \| ^2}{\{n-tr(F_\lambda) \}^2} \end{equation}

    \ref{FULLGCV} describes the full process of finding the optimal \textlambda.  Literature suggests a Newton method to optimize $V_g$ with respect to $log(\lambda)$ with $\widehat{\beta}_\lambda$ being obtained from the minimization of \label{FINALSMOOTH} as the full process of finding a optimal smooth function. In practice however has penalized likelihood maximization, in place of penalized least squares proven to be more useful. The algorithm that is used to maximize the penalized likelihood is penalized iteratively reweighted least squares (PIRLS) which proceeds as follows, where V is the function such that $var(y_i) = \phi V(\mu_i)/$ and $\mu_i = E(y_i)$. First initialize $\widehat{\mu_i} = y_i + \epsilon_i $ and $\eta_i = g(\mu_i)$ where $\epsilon_i$ is a small quantity (often 0) added to ensure that $g(\mu_i)$ exists.

    Then iterate the following steps to convergence.
    \begin{enumerate}
    \item form $z_i = g'(\widehat{\mu_i})(y_i − \widehat{\mu_i}) + \eta_i$ and $w_i = V(\widehat{\mu_i})^(1/2) g'(\widehat{\mu_i})$.
    \item putting the $wi$ in a diagonal matrix $W$, minimize the weighted version of expression \ref{FINALSMOOTH}:

    \begin{equation} \label{FINALSMOOTHW} \left \| Wz - WX\beta  \right \|^2 + \lambda \beta^T S\beta  \end{equation}


    with respect to $\beta$ to obtain $\widehat{\beta_i}$ and the updates $\widehat{\eta_i} =X\widehat{\beta}$, and $\widehat{\mu_i} =g^-1(\widehat{\eta_i})$.
    \end{enumerate}

    For moderate size data PIRLS will converge for each tiral $\lambda$. The main issue with PIRLS is the requirement to form the whole model X at each iteration. The difficulty is simply that the model matrix for the model can become too big: if n and p are respectively the number of rows and columns of the model matrix, and M is the number of smoothing parameters, then the memory requirements of GAM fitting methods are typically $O(Mnp^2)$. The aim of my bachelor thesis is to lay out a faster way of finding the smoothing term.



    \subsection{Generalized Additive Models for large data sets}
    \cite{bigdataGAM} introduces two extensions of the development methods for fitting GAMs. Those suggested methods aim at numerical optimization and parallelization.

    The numerical optimization propossed by \cite{bigdataGAM} rewrites the general cross validation step to not require the full model matrix X in each step. Woods describes a simple stategy how model matrix factorization can be used to avoid formation of the whole model matrix in each iteration.

    Now suppose that the model matrix is first QR decomposed into a column orthogonal $n * p$ factor Q and an upper triangular $p * p$ factor R so that $X=QR$: If we also form $f =QTy$ and then expression \label{FINALSMOOTH} becomes

    \begin{equation} \label{NEWGCV} \left \| f- R\beta \right \|^2 + \left \| r \right \|^2 + \sum_{j}^{ } \lambda_j \beta^T S_j \beta \end{equation}. \ref{NEWGCV} allows us to rewrite \ref{NEWGCV} in a form that does not require the full model matrix X.

    \begin{equation} \label{NEWNEWGCV} V_g(\lambda) = \frac{n\left \| f-R\widehat{\beta}_\lambda \right \| ^2 + \left  \| r \right \| ^2}{\{n-tr(F_\lambda) \}^2} \end{equation}.

    \label{NEWNEWGCV} brings a tramendous advantage over \label{FULLGCV} in terms of memory and computational usage. Receiving all relevant information to find \textlambda for a given $X$ only forming subsections $R$ $f$ and $||r||^2$ gives a significant advantage over the proposed method in \ref{FULLGCV}.

    The parallelization optimimization of \label{FULLGCV} envolves the computations of $R$ $f$ and $||r||^2$ in a way that only requires small, disjunct subsections of $X$. This allows us to distribute the computation by grouping $X$ into equally sized non-overlapping sets and allocate them to different processors.


    \subsection{Apache Spark}
    Distributed computing is a special interest of mine. I've had the pleasure of setting up a SparkR cluster and have been following its development closely. Apache Spark advertises itself as a fast and general cluster computing engine for large-scale data processing. He a overview of it's key features:
    \begin{itemize}
        \item fast, Apache Spark currently holds the world record in sorting 1 TB on data, beating the previus record help by Hadoop. Spark reaches this high speed by removing I/O to disc and maintaining all relevant in memory.
        \item general, Aparche Spark offers a high level API called Resilient Data Set. RDDs are designed to facilitate a resiliend, distributed data set as a optimal level of interaction with data. Each of the key features of RDDs are outlined below:
        \begin{itemize}
            \item Resilient: Every RDD is evaluated lazy and constructed from it's lineage, point to different previous RDD.
            \item Distributed: RDDs are partiioned accross many machines
            \item Data set: RDDs are ment to think about in terms of matrixes, this should feel very natural for people familar with data.
        \end{itemize}
        \item cluster computing, Apache Spark is build to be executed in a cluster enviroment like Mesos or Yarn. The management and design of these systems are not subject of my bachelor thesis.
        \item large scale data processing, the data managable by Apache Spark is limited by the amount of memory availble in a cluster.

    \end{itemize}

    \subsection{General additive models in Apache Spark}
    I strongly believe that Apache Spark offers an ideal enviroment for gitting GAMs for large data sets. The optimizations suggested by \cite{bigdataGAM} are optimal for an general purpose in-cluster computing engine for the following reasons:
    \begin{itemize}
        \item The iterative nature of PIRLS is optimal for Spark's in memory computing.
        \item Spark's high level RDD allows for a flexible data structure to implement the suggested methods.
        \item Spark is designed to facilitate the parallel nature of finding $R$ $f$ and $||r||^2$ in subblocks of $X$.
        \item Spark supports \cite{Breeze}, a linear algebra libraries for Scala with far reaching capabilities.
    \end{itemize}

    \section{Guiding Questions}

    The leading question for my bachelor thesis can be stated as:
    \paragraph{Can the distributed, numerically optimized, in-memory version of GAM fitting outperform previous benchmark implementation in model size, data size and fitting speed?}

    \paragraph{}
    Subquestions arising are:
        \begin{itemize}
        \item Can the suggested methods be implemented in Apache Spark?
        \item How much faster is a parallel variant against the sequential one?
        \item How much faster is the Spark in-memory variant against the R one?
        \item How do the limit for data and model size change?
        \item How much does horizontal scaling affect performance?
        \end{itemize}


    \subsection{What is novel in this approach?}
    The suggested approach has been implemented in high-level statistical languages, proving its effectiveness. The suggested methods have not been implemented in an enviroment designed for in-memory cluster computing. As I lined out earlier the iterative nature of the proposed methods is well suited for a distributed in-memory enviroment and hence should be discoved

    \subsection{When is the thesis regarded as completed?}
    The thesis is considered finished, once the optimized version of GAM fitting has been implemented in Spark made available in SparkR and compared to other implementation of GAMs.

    \paragraph{Additional tasks}
        \begin{itemize}
            \item Develop a deep theoretical unterstanding of GAMs with \cite{gamBook}
            \item Develop a better understanding of used matrix decompositions with \cite{strang09}
            \item Set up an Apache Spark enviroment
            \item Cordinate with my employer to facilitate support
        \end{itemize}

    \subsection{Which tasks are realistic to be accomplished within the scope of this thesis?}
    I strongly belive that the suggested methods can be implemented in Apache Spark within this thesis. In an ideal situation I would like to make my implementation available to the Apache project.

    \section{Time table}
    My time plan is visible at table \ref{timetable}.
    \begin{table}[h]
        \begin{center}
        \caption{Time table} % http://www.latex-tutorial.com/tutorials/beginners/lesson-8/
        \label{timetable}
        \begin{tabular}{rrl}
            \toprule
            CW & thesis work \\
            \midrule
            45  & Develop a deep theoretical unterstanding of GAMs and used numerical methods \\
            46  & Develop a deep theoretical unterstanding of GAMs and used numerical methods \\
            47  & Develop a deep theoretical unterstanding of GAMs and used numerical methods \\
            48  & Set up an Apache Spark Enviroment \\
            49  & Start implementing  \\
            50  & Implementation \\
            51  & Winter break \\
            52  & Winter break \\
            1  & Imlementation and Exams \\
            2  & Imlementation and Exams \\
            3  & Imlementation and Exams \\
            4  & Imlementation and Exams \\
            5  & Break \\
            6  & Finish implementation \\
            7  & Start writing \\
            8  & writing \\
            9  & writing \\
            10  & writing \\
            11  & writing \\
            12  & writing \\


        \end{tabular}
        \end{center}
    \end{table}
    \newpage

    \bibliography{expose}    % reference to expose.bib

    \newpage

\end{document}
