\documentclass{article}

% Latex Tutorial beginners: http://www.latex-tutorial.com/tutorials/beginners/
\newcommand{\comment}[1]{}

\newcommand{\seq}[0]{$P_{s}$}
\renewcommand{\mp}[0]{$P_{m}$}
\newcommand{\ndp}[0]{$P_{np}$}
\newcommand{\note}[1]{{\tiny (#1)}}

\newcommand{\algo}[0]{Split\&Merge-Segmentation}

% Packages explained - Adding more functions to LATEX http://www.latex-tutorial.com/tutorials/beginners/lesson-3/
\usepackage[utf8]{inputenc}     % Zum verwenden von ä,ü und ö: http://en.wikibooks.org/wiki/LaTeX/Special_Characters
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}

\usepackage[margin=1.3in]{geometry}

% Erzeugen des PDF (ohne References):
% Strg + Alt + 1 in Gedit

% Erzeugen des PDF (mit References):
% Strg + Alt + 1 in Gedit
% $ bibtex expose
% Zweimal: Strg + Alt + 1 in Gedit

% fügt die Literatur auch zum Inhaltsverzeichnis hinzu (ist so nicht default)
\usepackage[nottoc,notlot,notlof,numbib]{tocbibind}

% Adding a bibliography http://www.latex-tutorial.com/tutorials/beginners/lesson-7/
\bibliographystyle{apalike}


% Nested numbered enumerations, eg. 1.1, 1.2: http://tex.stackexchange.com/questions/78842/nested-enumeration-numbering
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}


\title{
    Fitting Generalized Additive Models\\ for large data sets with Apache Spark \\[7pt]
    \large An exposé for a bachelor thesis
}
\date{24.10.2015}
\author{Kai Thomas Brusch}


\begin{document}

    \pagenumbering{arabic}

    \maketitle

    \paragraph{Summary}

    This exposé describes content, goals and motivation of my bachelor thesis titled "Fitting General Additive Models for large data sets with Apache Spark".

    \paragraph{Contact} \texttt{kai.brusch@gmail.com}

    \paragraph{Location} Hochschule für Angewandte Wissenschaften Hamburg
    \paragraph{Department} Dept. Informatik
    \paragraph{Examiner} Prof. Dr. Michael Köhler-Bußmeier
    \paragraph{Second examiner} TBA

    \newpage

    \tableofcontents

    \newpage

    \section{Introduction}

    \subsection{Context}
    Regression analysis have become a corner stone of modern statistical analysis. Discovering the relationship between dependeant and independat variables offers great insight into the underlying mechanics and provides far reaching predictive capabilities. The general additive model (GAM) describes a modelling approach which explores the relationship between dependant and independant variables as the sum of smoothing functions. This modeling approach offers more flexiblity and power than the traditional generalized linear model and has been succesfuly established in academia and industry. However, the additional power of GAMs comes at the cost of additional computational effort, placing strict constraints on the model and data size. Modern literatur suggest two optimization to eliviate this constraint: parallelization of computation and numerical optimization.

    Apache Spark offers a general purpose cluster computing engine. It's general and distributed nature offers the ideal enviroment to facilitate .

    \subsection{General Linear Model}
    I am fascinated by the language as it enables

    \subsection{Generalized Linear Model}

    \subsection{Generalized Additive Model}

    \subsection{Apache Spark}
    Image Processing is a topic of high personal interest.
    I particularly liked the elective course "Robot Vision" (Prof. Dr. Meisel).
    Image Processing Algorithms have a natural tendency to be applicable in Parallel Computing (as many algorithms end up on high-performance GPUs).
    Therefore, I think such a focus is adequate.
    I am also focusing on Image Processing Algorithms because I want to express my interest on it.

    \subsection{General approaches in Parallel computing}
    Generally recognized approaches are:
    \begin{itemize}
        \item Concurrent programming (threads \& locks)
        \item Flat data Parallelism (\texttt{map/reduce})
        \item GPU Parallelism (CUDA,OpenCL, mainly matrix/vector operations)
        \item and in the Haskell world: \cite{Marlow2012Parallel}
        \begin{itemize}
            \item Algorithm + Strategy = Parallelism, a task-based approach \cite{Trinder1998Algorithm}
            \item The Par Monad, a dataflow approch, \cite{Marlow2011Monad}
            \item Repa, a regular-array data-parallel approach, \cite{Keller2010Regular}
            \item Accelerate, a GPU approach, \cite{McDonell2013Optimising}
            \item Nested Data Parallel, a parametric nested-data-parallel approach, \cite{Chakravarty2007Data}
        \end{itemize}
    \end{itemize}


    \subsection{Leading question}

    \paragraph{Can the distributed, numerically optimized, in-memory version of GAM fitting beat previous benchmark implementation in model size, data size and fitting speed?}

    \paragraph{}
    Subquestions arising are:
        \begin{itemize}
        \item How much faster is a parallel variant against the sequential one?
        \item How much faster is the Spark in-memory variant against the R one?
        \item What is the new limit for data and model size?
        \end{itemize}

    \subsection{Specific Approaches in Haskell and a comparision}
    The following is a brief comparision of the candidate approaches for my thesis.
    \begin{itemize}
    \item \textbf{Repa: Regular, shape-polymorphic Parallel Arrays}
        \begin{itemize}
            \item[+] Retains high level of abstraction
            \item[+] Highly efficient mashine-code (e.g. fusioning)
            \item[+] Feels like conventional functional programming
            \item[+] Can compile to OpenCL/CUDE for high performance GPU execution
            \item[+] Multi-dimensional regular arrays (e.g. matrices)
            \item[-] No support for irregular data structures (e.g. sparse-matrices, trees, graphs, recursion)
            \item[\textbullet] Recommended for matrix-/pixel-algorithms
        \end{itemize}

    \item \textbf{ Algorithm + Strategy = Parallelism}
        \begin{itemize}
            \item[+] Algorithm and Parallel evaluation strategy are entirely independent
            \item[+] (Therefore) Highly irregular algorithms can be paralellized
            \item[+] Feels like conventional functional programming
            \item[+] Very easy to use
            \item[-] The programmer has to ensure subtle pre-/post-conditions for successful parallelization
            \item[-] Cannot compile to OpenCL/CUDE for high performance GPU execution
            \item[\textbullet] Recommended as simple-parallization approach
        \end{itemize}



    \subsection{Deciding on an approach}

    I have decided to implement the fitting of GAMs in Apache Spark. My choice will be explained now:
    \paragraph{Nested Data Parallel}
        Spark is fast
        Iterative nature is well suited for in-memory



    \subsection{What is novel in this approach?}
    The suggested approach has been implemented in high-level statistical languages, proving its effectiveness. The suggested methods have not been implemented in an enviroment designed for in-memory cluster computing. As I lined out earlier the iterative nature of the proposed methods is well suited for a distributed in-memory enviroment and hence should be discoved

    \subsection{When is the thesis regarded as completed?}
    The thesis is considered finished, once various strategies of
    implementing a chosen algorithm (e.g. \algo) have been
    implemented and compared in effectiveness/efficiency/human-workload.

    \subsection{Which tasks have to be completed for this thesis?}
    My work is defined by creating the programs \seq, \mp and \ndp and comparing them.
    The focus is on showing the various transformations applied in NDP, which the programmer would have had done manually.

        $A_{K} :=$ A conceptional Problem \note{e.g. Sorting}

        $A_{S} :=$ A sequential Algorithm to $A_{K}$ \note{e.g. Mergesort}

        $P_{P} :=$ A parallel Algorithm to $A_{K}$ \note{e.g Parallel Mergesort}

        \seq $:=$ An ordinary Haskell implementation of $A_{S}$ \note{e.g. using Lists}

        \mp $:=$ A manually-parallelized implementation of $A_{P}$ \note{e.g using explicit threads}

        \ndp $:=$ A Nested-Data-Parallel implementation of $A_{P}$ \note{using parallel arrays}

    \paragraph{Additional tasks}
        \begin{itemize}
            \item Read further papers on how NDP is implemented (especially. vectorization and fusioning)
            \item Read futher literature on how to make quantified comparisions of the programs (e.g. parallel complexity, benchmarks, etc...)
            \item Decide whether I will work with the true Haskell-Core code generated in \ndp or apply the flattening and fusioning transformations manually.
                The first variant is the actual source code that will later be executed - but it is hard to read. \footnote[1]{See \texttt{DotP.hs} and \texttt{DotP.vect.hs} at \href{https://github.com/GollyTicker/Nested-Data-Parallel-Haskell/tree/0e8d3df0d8084a01b007b27debda2b64247a254d}{my github repo}. }
                The implementation of DPH is less developed than its theoretical background.
                The second variant is easier to work with and simpler to present, however it will almost certainly be less optimized than the actual code.
            \item Decide which algorithm(s) is/are to be used as examples. They should be easily visualizable and should have irregular behaviour.
                Connected-Components-Labeling on images is such an candidate. Combining multiple algorithms into
                a pipeline is also attractive, since cross-algorithm fusionsing and optimization is where Haskell can shine truely.
            \item Learn how make benchmarks/create runtime statistics
        \end{itemize}

    \subsection{Which tasks are realistic to be accomplished within the scope of this thesis?}
    I am optimistic that these task can be accomplished in a timeframe of 2 months. However, I might need to keep a tight focus or
    choose a simple (and small) algorithm to keep the task accomplishable.

    \subsection{Time plan}
    My time plan is visible at table \ref{timetable}.
    \begin{table}[h]
        \begin{center}
        \caption{Time table} % http://www.latex-tutorial.com/tutorials/beginners/lesson-8/
        \label{timetable}
        \begin{tabular}{rrl}
            \toprule
            CW & monday & thesis work \\
            \midrule
            17 & 20.04 & reading remaining papers, reading parallel complexity theory \\
            18 & 27.04 & deciding on an algorithm, learning benchmarking  \\
            19 & 4.05  & implementing \seq and \mp \\
            20 & 11.05 & implementing \ndp \\
            21 & 18.05 & vectorizing and optimizing \ndp / understanding generated core \\
            22 & 25.05 & analysis and benchmarking \\
            23 & 1.06  & \textit{puffer} \\
            24 & 8.06  & \textit{puffer} \\
            25 & 15.06 & Begin to write down, prepare for exams\\
            26 & 22.06 & Writing..., prepare for exams \\
            27 & 29.06 & Writing..., prepare for exams \\
            28 & 6.07  & Prepare Colloquium, Writing..., exams week 1 \\
            29 & 13.07 & Prepare Colloquium, Finalize writing, exams week 1 \\
            30 & 20.07 & Colloquium and Release \\
            31 & 27.07 & Last week for Colloquium and Release \\
            32 & 3.08  & Fin \texttt{:D} \\
        \end{tabular}
        \end{center}
    \end{table}

    \section{Draft - Contents}

        The following presents a draft structure of my thesis.
        Since the algorithm I am going to implement is not decided yet, I am using "\algo" as its placeholder.

        \begin{enumerate}

        \item Introduction
            \begin{enumerate}
            \item Context
            \item Goal
            \item Structure
            \end{enumerate}

        \item Basics
            \begin{enumerate}
            \item Parallel Computing and Complexity
            \item Haskell
            \item Nested Data Parallelism
                \begin{enumerate}
                \item Parallel Arrays
                \item Sparse Matrices - An Example
                \item Execution model
                \end{enumerate}
            \item $A_{K}$: \algo
            \end{enumerate}

        \item \seq: Sequential \algo
            \begin{enumerate}
            \item Implementation
            \item Runtime analysis {\tiny (e.g. sequential/parallel complexity)}
            \item Benchmark
            \end{enumerate}

        \item \mp: Manually-parallel \algo
            \begin{enumerate}
            \item Implementation
            \item Runtime analysis {\tiny (e.g. sequential/parallel complexity)}
            \item Benchmark
            \end{enumerate}

        \item \ndp: Nested-Data-Parallel \algo
            \begin{enumerate}
            \item High-level Implementation
            \item Transformations
                \begin{enumerate}
                \item Non-Parametric representation
                \item Lifting
                \item Vectorization
                \item Fusioning
                \end{enumerate}
            \item Final low-level form
            \item Runtime analysis {\tiny (e.g. sequential/parallel complexity)}
            \item Benchmark
            \end{enumerate}

        \item Evaluation
            \begin{enumerate}
            \item Sequential vs. Parallel
            \item Manually-Parallel vs. Nested-Data-Parallel
            \end{enumerate}

        \item Conclusion
            \begin{enumerate}
            \item Effectivenesss of Nested Data Parallel
            \item Related work {\tiny (on parallel computing in Haskell)}
            \item Future work
                \begin{enumerate}
                    \item Alternate Algorithms
                    \item Best of Repa and NDP
                    \item Distributed NDP
                    \item NDP on GPUs
                \end{enumerate}
            \end{enumerate}
        \end{enumerate}

    \newpage

    \bibliography{expose}    % reference to expose.bib

    \newpage

    \section{Personal notes}

    \begin{itemize}
        \item Zeitplan für 2.5 Monate
        \item English als Ausarbeitungssprache
        \item Wahl eines Bildverarbeitungsalgorithmus welcher nicht offensichtlich parallelisierbar ist
        \item A collection of image processing algorithms with varying degree of parallization: \note{from a discussion with Prof. Meisel}
            \begin{enumerate}
                \item 99\%: Median, Faltungsmasken
                \item 80\%: Parallele Kantenverfolgung, $\mu$-Momente
                \item 70\%: Rekursive Algorithmen (Connected Components Labeling, Split&Merge Segmentierungsverfahren,...)
                \item 60\%: Shortest Paths (e.g. Seam Carving)
                \item 10\%: Sequentielle Kantenverfolgung
                \item 0\%: Zufallsgenerator
            \end{enumerate}
    \end{itemize}


\end{document}
