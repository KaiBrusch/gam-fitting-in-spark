\documentclass{article}

% Latex Tutorial beginners: http://www.latex-tutorial.com/tutorials/beginners/
\newcommand{\comment}[1]{}

\newcommand{\note}[1]{{\tiny (#1)}}

% Packages explained - Adding more functions to LATEX http://www.latex-tutorial.com/tutorials/beginners/lesson-3/
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{textgreek}
\usepackage{listings}
\usepackage{graphicx}

\usepackage[margin=1.3in]{geometry}

% fügt die Literatur auch zum Inhaltsverzeichnis hinzu (ist so nicht default)
\usepackage[nottoc,notlot,notlof,numbib]{tocbibind}

% Adding a bibliography http://www.latex-tutorial.com/tutorials/beginners/lesson-7/
\bibliographystyle{apalike}


% Nested numbered enumerations, eg. 1.1, 1.2: http://tex.stackexchange.com/questions/78842/nested-enumeration-numbering
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}


\title{
    Fitting Generalized Additive Models for\\ very large datasets with Apache Spark \\[7pt]
    \large Chapter Excerpt
}
\date{24.12.2015}
\author{Kai Thomas Brusch}

\begin{document}

    \pagenumbering{arabic}

    \maketitle

    \paragraph{Summary}

    This document contains three introductory chapters for my bachelor thesis titled "Fitting General Additive Models for very large datasets with Apache Spark".

    \paragraph{Contact} \texttt{kai.brusch@gmail.com}

    \paragraph{Location} Hochschule für Angewandte Wissenschaften Hamburg
    \paragraph{Department} Dept. Informatik
    \paragraph{Examiner} Prof. Dr. Michael Köhler-Bußmeier
    \paragraph{Second examiner} Dipl.-Math. Markus Schmaus

    \newpage

    \tableofcontents

    \newpage

    \section{Linear models}

    \subsection{Introduction to linear models}
    Linear models are statistical models in which an univariate response is modeled as the sum of a ‘linear predictor’ and a zero mean random error term. The linear predictor depends on some predictor variables $y$, measured with the response variable x, and some unknown parameters $\beta$ plus an error term $\epsilon$, which must be estimated. This process is formally stated for a given row  $i$ of data as: \cite{gamBook} \cite{bigdataGAM} \cite{spark}

    \begin{equation}  \label{linModel} y\textsubscript{i} = \beta x\textsubscript{i} + \varepsilon\textsubscript{i} \end{equation}

    The are many choices for $\beta$ and finding the best possible $\beta$ stands at the heart of the following chapter. A key feature of linear models is that the linear predictor depends linearly on these parameters. Statistical inference with such models is usually based on the assumption that the response variable has a normal distribution. Linear models are used widely in most branches of science and an example is given in the next section.

    \subsection{Example of a linear model}
    Before describing the relevant theory I would like to give an example of a simple linear model. Lets say we have a data set called mtcars thats describes cars with respect to miles per galon (mpg) and horse power(hp). After careful thought and the examination of a scatter plot I believe that there is a linear relation ship between the the miles per galon and the horse power of a car. I also believe that miles per gallon follow normal distribution. I would try to explain the relationship between hp and mpg as a linear model takes miles per galon as the dependant variable and horse power as the independant variable. This model decription written in R yields the following line of code:

    \begin{lstlisting}[language=R]
    model <- lm(data=mtcars,hp ~ mpg)

    summary(model)
    \end{lstlisting}

    The summary gives me the estimated model and we can see the slope and the insect of function explaining the relationship between dependant and independant variable:
    \begin{lstlisting}[language=R]
    Call:
    lm(formula = hp ~ mpg, data = mtcars)

    Coefficients:
    (Intercept)          mpg
       297.688       -8.022
    \end{lstlisting}

    Our model describes hp power as $-8.022*mpg +\ 297.688$. We can see that the more hp a car has the fewer mpg does it offer. The red line in the plot is our estimated function that illustrates the estimated model

    \begin{figure}[ht]
    \centering
        \includegraphics[width=.5\textwidth]{linplot.png}
        \caption{$-8.022*mpg +\ 297.688$}
    \label{fig1}
    \end{figure}

    The natural question that arises is: how to we estimate the line and the error of the chart above? The relevant method is called ordinary least squares and will be introduced in the next section.

    \subsection{Ordinary least square estimation of $\beta$}

    We are now looking at methods of finding $\beta$. Ideally we want to choose a $\beta$ that produces a line through our data points with minimal distance between our points and our estimated line. More precicely we are looking to estimate a $\beta$ that minimizes the squared distance between an estimated $\beta$ times the given $x$ and $y$. We are squaring the distance to normalize negative and positive differences. This distance formaly describes as S:

    \begin{equation} \label{sumSquares} \mathbf{S} =  \sum_{i=1}^{n}(y\textsubscript{i} - x\textsubscript{i}\beta)^{2} \end{equation}

    The close our S gets to 0 the better our line fits the data. The  Markov-Gauss Theorem states that the minimization of S yields $\widehat{\beta}$ which is the best possible estimation for $\beta$. This shall be discussed n the next chapter. \ref{linModel} represents the univariate case where y is explained with only one variable. The process of minimizing S is commonly refered to as ordinary least squares (OLS).

    There are two ways to think about estimating $\beta$, thinking off OLS in terms of calulus on functions allows us to compute $\widehat{\beta}$ for the univariate case while computing $\widehat{\beta}$ for multiple independant variables requires linear algebra. From the calculus perspective we can see OLS as function with two parameters: $S$ and $\beta$. Minimizing S equates to taking the partial derivative of $S$ with repsect to $\beta$. This is the most common approach and offers insight by stating S as the following equation:

    \begin{equation} \label{partialDer} \frac{\partial S}{\partial \beta} = - \sum_{i=1}^{n} 2x_i(y_i-x_i\beta) \end{equation}

    Rewriting the partial dereviative to yiel $\widehat{\beta}$ gives a very good idea of $\beta$

    \begin{equation}  - \sum_{i=1}^{n} 2x_i(y_i-x_i\widehat{\beta}) = 0  \end{equation}

    \begin{equation} - \sum_{i=1}^{n} x_iy_i-\widehat{\beta} \sum_{i=1}^{n} x_i^2 = 0  \end{equation}

    \begin{equation} \widehat{\beta} = \sum_{i=1}^{n} x_iy_i /\sum_{i=1}^{n} x_i^2 \end{equation}


    Minimizing S w.r.t. $\beta$ to compute $\widehat{\beta}$ is a reasonable approach when dealing with one independant variable. However, almost all relevant applications involve much more than one independant variable and require linear algebra. To estimate $\widehat{\beta}$ with OLS for multiple independant variables involves rephrasing the questions in terms of linear algebra. First we have to state the process of finding $\beta$ as a linear combination problem whichs equates to asking what linear combination of column vectors of our model matrix $X$ and our vector $\beta$ of unknown coefficents yields the vector of $y$. Formally:

    \begin{equation} \label{matrix} X\beta = y \end{equation}

    Now that we have restated OLS as a matrix problem we can apply some linear algebra to find $\widehat{\beta}$. First we can restate the problem of finding $\beta$ as finding the unkown vector $\beta$ equates to finding a linear combination of our column vectors of $X$ with $\beta$ that result in $y$. Finding this linear combination is highly dependant on the properties of $X$. Taking a look at relevant model matricies $X$ will show that they almost exclusively consist of m rows and n columns with $m > n$. Being $m > n$ implies that the matrix is not symetric and not invertible. Given that $X$ has more rows than columns we can think of $X\beta = y$ as a system of equations with more equations than variables which causes this sytem of equation to have no solution. We have to stress the fact that the sytem of equations of \ref{matrix} does not have a solution because there is no possible selecton for $\beta$ that lies in the column vector space of $X$. Whilst the nature of $X$ makes finding $\beta$ impossible we can find an estimation $\widehat{\beta}$ of $\beta$ by projecting it back into the column space of $X$.  The solution becomes to multiply by $A^T$The proper projection $p$ is defined as $p = X\widehat{\beta}$. The process of projecting of finding the projection involves multiplying by the transpose of the model matrix yielding:

    \begin{equation} X^T X \widehat{\beta} = X^T y  \end{equation}

    This projection comes however at the cost of an error term:

    \begin{equation} y = X\widehat{\beta} + \epsilon  \end{equation}
    \begin{equation} \epsilon = y-X\widehat{\beta} \end{equation}

    The questions that now arises is: how to we make $\epsilon = y-X\widehat{\beta}$ as small as possible? Using algebra we can split the vector $\beta$ into two parts. One part in the column space is our projection $p$ and the perpendiclar part in the nullspace of $A^T$ which the $\epsilon$. It is essential to remember that the column space is always perpendicular to the nullspace of $A^T$. The solution to $X\beta = p$ leaves the least possible error $\epsilon$, returning to the previously stated method of least squares, but this time in the world of linear algebra:


     \begin{equation} \left \| X\beta -y \right \|^2 = \left \| X\beta - p \right \|^2 + \left \| \epsilon \right \|^2 \end{equation}

     This is the law $c^2 = a^2 + b^2$ for a right angle. The vector $X\beta-p$ in the column space is perpendicular to $\epsilon$ in the nullspace of $A^T$. We reduce $X\beta-p$ to zero choosing $\beta$ to be $\widehat{\beta}$, this leaves us with the smallest possible error vector $\epsilon$. The projection leaves us with an invertible matrix that can be solved by usual elimination. The least squares solution $\widehat{\beta}$ makes $\epsilon = X\beta$ as small as possible.



    \subsection{Gauss-Markov Theorem}
    So far we have introduced two different methods on how to estimate $\beta$. The calculus way for univariate data and the linear algebra way for multivariate data, both estimated $\beta$ with an approximate $\widehat{beta}$ which seeks to minimize the squared error of actual and estimate. While squared error seems like the an obvious choice we have no formal reason to prefer it to other measures . This section will lay out the formal proof that shows why $\widehat{\beta}$ is the best possible estimator.\cite{gamBook} states that there exists no estimator with lower variance than least squares estimation. The


    \subsection{Estimating $\widehat{\beta}$ with orthogonal decomposition}
    The previously suggested method lends a great tool to think about the univariate least squares technique. But the suggested methods is rarely applied
    Any m by n matrix X with independant columns can be factored into QR. The m by n matrix Q has orthonormal columns and the square matrix R is upper triangular with positive diagonal. $X^T X$ equals $R^T Q^T QR = R^T R$ simplifying the leas squares equation to $Rx = Q^T \beta$; allowing us to multiply by $R^T$ instead of $X^T$ for form the square matrix.  This additional simplicity allows us to restate the problem of least squares for matrices using QR decomposition as:

    \begin{equation} R^T R \widehat{\beta} = R^TQ^T y  \ or \   R\widehat{\beta}=Q^T y \ or \  \widehat{\beta} = R^{-1} Q^T y \end{equation}

    A very important property of orthogonal matrices is that their multiplication with a  vector is ideponent with respect to the lenght of the vector.

    Instead of multipliying with a fully formed model matrix X we only multiply with small subset, thus reducing the amount of mulipilications and memory requirements drasticaly.

    The QR decomposition is formally stated in the appendix and will appear as an essential part later in this thesis.


    \section{Generalized Linear Models}
    \subsection{Introduction to Generalized Linear Models}
    \cite{gamBook} describes Generalized Linear Models (GLMs) as an extension of the general linear model with the ability to model more expotential family response disributions. While linear models with OLS estimation only allow for a normal distributed response, GLMs allows to model the reponse variable from any arbitrary expotential family. The expotential family of distributions contains many distibutions that are very useful for pracical model. A formal description of it's basis structure can be given as

        \begin{equation} g(\mu_i) = X_i \beta_i \end{equation}


    Where $\mu_i \equiv E(Y_i)$ and $Y_i$ is distibuted according to some expotential family. Its members include but are not limited to Poisson, Bionomial, Gamma and Normal Distributions. Every expotential family distribution has a link function g(). $X_i$ is the $i^th$ row of a model matrix X and $\beta$ is a vector of unknowns parameters. The link functions allows us to model an expotential function ins terms of a linear link function. To account for expotential familty distributions comes at a cost however: while OLS was sufficient for estimating $\beta$ for normal distributed data now have to generalize this notion to acount for an arbitrary amount of distribution parameters. The generalization of OLS is called maximum likelihood estimation (MLE) and involves an iterative method called iterative re-weighted least squares (IRLS). An important practical feature of generalized linear models is that they can all be fit to data using IRLS, independant of response variable distribution.

    \subsection{Example of a Generalized Linear Models}
    MODEL SOME STUFF BASED ON SOME STUFF
    \subsection{Maximum likelihood estimation}
    Parameter estimation stands at the heart of all statistical models. Random variables and their distribution allows us to explain a variables behavior based on their parameters. Once a model is specified with its parameters, and data have been collected, one is in a position to evaluate its goodness of fit, that is, how well it fits the observed data. Goodness of fit is assessed by finding parameter values of a model that best fits the data procedure called parameter estimation.The OLS linear model estimated the $\mu$ and $\sigma$ for $N( \mu ,\sigma^2)$ by minimizing $S$. This method of parameter estimation is specific to a normal distributed response and needs to be generalized to account for expotential family distributions. Expotential family distributions have diffent number of parameters and parameter meaning, cf.  $P(\lambda)$ $G(K,\theta)$. Maximum likelihood estimation provides a single framework to allow parameter estimation for any of the expotential family distributions.

    From a statistical analysis point of view the vector $y$ of observed data is a random sample from an unknown population. The goal of maximum likelihood estimation is to find the parameters of the given distribution that most likely has produced this sample. This process is discribed with a probability density function (PDF) $f()$ of observed data $y$ given a parameter $w$: $f(y|w)$. If individual observations, $y_i$’s, are statistically independent of one another, then according to the theory of probability, the PDF for the data y given the parameter vector w can be expressed as a multiplication of PDFs for individual observations.

    \begin{equation} f(y|w) = f((y_1, y_2,...,y_n) | (w_1,w_2, ... w_n )) = f_1(y_1|w_1)*f_2(y_2|w_2) ... f_n(y_n|w_n) \end{equation}

    Given a set of parameter values, the corresponding PDF will show that some data are more probable than other data. In reality the data is already given and we are searching for the parameters of the distribution that mostlikely produced the data. We thus inverse our function $f(y|w)$ to $L(w|y)$ to produce the likelihood of y given the parameters w. The principle of maximum likelihood estimation, originally developed by R.A. Fisher in the 1920s, states that the desired probability distribution is the one that makes the observed data ‘‘most likely,’’ which means that one must seek the value of the parameter vector that maximizes the likelihood function $L(w|y)$: The resulting parameter vector, which is sought by searching the multi-dimensional parameter space, is called the MLE estimate.

    Theoretically MLE estimates need not exist nor be unique. But if they exist and are unique they can be estimated. For computational convenience, the MLE estimate is obtained by maximizing the log-likelihood function, $ln(L(w|y))$: This is because the two functions, $ln(L(w|y))$ and $L(w|y)$; are monotonically related to each other so the same MLE estimate is obtained by maximizing either one and the log-likelihood is prefered for obvious reasons. Assuming that the log-likelihood function, $ln(L(w|y))$ is differentiable, if wMLE exists, it must satisfy the following partial differential equation known as the likelihood equation:

    \begin{equation} \frac{\partial ln L(w|y)}{\partial w_i} = 0 \end{equation}

    This is because the definition of maximum or minimum of a continuous differentiable function implies that its first derivatives vanish at such points. The likelihood equation represents a necessary condition for the existence of an MLE estimate. An additional condition must also be satisfied to ensure that ln $ln(L(w|y))$ is a maximum and not a minimum, since the first derivative cannot reveal this. To be a maximum, the shape of the log-likelihood function should beconvex (it must represent a peak, not a valley) in the neighborhood of wMLE: This can be checked by calculating the second derivatives of the log-likelihoodsand showing whether they are all negative.

    \begin{equation} \frac{\partial^2 ln L(w|y)}{\partial w^2_i} < 0 \end{equation}

    In practice, however, it is usually not possible to obtain an analytic form solution for the MLE estimate, especially when the model involves many parameters and its PDF is highly non-linear. In such situations, the MLE estimate must be sought numerically using nonlinear optimization algorithms. The basic idea of nonlinear optimization is to quickly find optimal parametersthat maximize the log-likelihood. This is done by searching much smaller sub-sets of the multi-dimensional parameter space rather than exhaustively searching the whole parameter space, which becomes intractable as the number of parameters increases. The ‘‘intelligent’’ search proceeds by trial and error over the
    course of a series of iterative steps. Specifically, on eachiteration, by taking into account the results from the previous iteration, a new set of parameter values is obtained by adding small changes to the previous parameters in such a way that the new parameters are likely to lead to improved performance. Different optimization algorithms differ in how this updating routine is conducted. This process is called

    \subsection{Fitting generalized linear models}
    Penelized Iterative Reweighted Least Square estimation

    \section{Generalized Additive Models}
    \subsection{Introduction to Generalized Additive Models}
    Generalized Additive Models (GAMs) extends the GLM by specifying the linear prediction in terms of the summation of smooth functions. This allows for a more flexible modeling of the influence for each explanatory variable. The gained flexibility comes at the cost of additional questions concerning the smooth function:

    GAMs are fomally described by the following equation:

    \begin{equation} \label{GAM} g(\mu\textsubscript{i}) = \mathbf{X} \textsubscript{i} \Theta + f\textsubscript{1}(x\textsubscript{1i}) + f\textsubscript{2}(x\textsubscript{2i}) + f\textsubscript{3}(x\textsubscript{3i}, x\textsubscript{4i}) ... \end{equation}

    \ref{GAM} explains $y_i$ as the model matrix for this row and the smooth functions $f_j(x_1j)$ of the x values for this row. $X_i$ is a row of the model matrix with parametric component $\theta$. Unlike the linear model we can now  specify a smooth function for each explanatory variable. This proves to be way more flexible than only allowing for a constant influence per explanatory variable. The natural question that arises now are: How do I find proper smoothing functions? Finding the right smooth function stands at the heart of GAM fitting and can be best illustrated in the univariate case \ref{univariateSmooth}.

    \subsection{Generalized Additve Model example}
    WOODS HELP ME!!!
    \subsection{Smoothing Functions}
     \begin{equation} \label{univariateSmooth} y_i = f(x_i) + \epsilon_i \end{equation}

     Smooth functions form a vector space, which can be approximated using a linear basis.Only allowing linear basis allow us to heavily leverage the theory already developed for linear models and $S$ as the optimal model fit. For the sake of illustration we assume that \ref{univariateSmooth} can be rewriten as the following equation if $b_i(x)$ is the $ith$ basis function:

    \begin{equation} \label{smoothBase} f(x) = \sum_{i=1}^{q} b\textsubscript{i} (x) \beta \textsubscript{i} \end{equation}

    In \ref{smoothBase} we already know $f()$ is linear in regard to \ref{smoothBase}. We now have to specify a basis function to represent $bi$. We can choose from many basis functions for $bi$, each with advantages and disadvantages. A common choice however is a fourth order polynomial basis function. \ref{smoothBase} represented by a fourth order polynomial yields the following model:

    \begin{equation} \label{poly4th} f(x) = \beta \textsubscript{1} + x\beta \textsubscript{2} + x^2\beta \textsubscript{3} + x^3\beta \textsubscript{4} + x^4\beta \textsubscript{5}  \end{equation}

    Applying \ref{poly4th} to \ref{univariateSmooth} we get the modeling of $y_i$ as the sum of smoothing functions.

    \begin{equation} \label{poly4thGAM}  y_i = \beta_1 + x_i\beta_2 + x_i^2\beta_3+ x_i^3\beta_4 + x_i^4\beta_5 + \epsilon_i \end{equation}
    \subsection{Regression Splines}
    \subsection{Smoothing Parameter estimation}
    \subsection{Fitting Generalized Additive Models}

    \section{Matrix Algebra}
    \subsection{Orthogonal Matrices}
    We are concerned with matrices that do have orthogonal column vectors, severly concerned
    \subsection{QR decomposition}
    We want to decompose a matrix A in two parts, one orthonormal and one upper triangular, X = QR


    \newpage

    \bibliography{thesis}    % reference to thesis.bib

    \newpage

\end{document}
