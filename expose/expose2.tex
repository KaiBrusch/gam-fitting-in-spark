\documentclass{article}

% Latex Tutorial beginners: http://www.latex-tutorial.com/tutorials/beginners/
\newcommand{\comment}[1]{}

\newcommand{\note}[1]{{\tiny (#1)}}

% Packages explained - Adding more functions to LATEX http://www.latex-tutorial.com/tutorials/beginners/lesson-3/
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{textgreek}

\usepackage[margin=1.3in]{geometry}

% Erzeugen des PDF (ohne References):
% Strg + Alt + 1 in Gedit

% Erzeugen des PDF (mit References):
% Strg + Alt + 1 in Gedit
% $ bibtex expose
% Zweimal: Strg + Alt + 1 in Gedit

% fügt die Literatur auch zum Inhaltsverzeichnis hinzu (ist so nicht default)
\usepackage[nottoc,notlot,notlof,numbib]{tocbibind}

% Adding a bibliography http://www.latex-tutorial.com/tutorials/beginners/lesson-7/
\bibliographystyle{apalike}


% Nested numbered enumerations, eg. 1.1, 1.2: http://tex.stackexchange.com/questions/78842/nested-enumeration-numbering
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}


\title{
    Fitting Generalized Additive Models for\\ very large datasets with Apache Spark \\[7pt]
    \large An exposé for a bachelor thesis
}
\date{31.10.2015}
\author{Kai Thomas Brusch}


\begin{document}

    \pagenumbering{arabic}

    \maketitle

    \paragraph{Summary}

    This exposé describes content, goals and motivation and gives a general introduction to my bachelor thesis titled "Fitting General Additive Models for very large datasets with Apache Spark".

    \paragraph{Contact} \texttt{kai.brusch@gmail.com}

    \paragraph{Location} Hochschule für Angewandte Wissenschaften Hamburg
    \paragraph{Department} Dept. Informatik
    \paragraph{Examiner} Prof. Dr. Michael Köhler-Bußmeier
    \paragraph{Second examiner} TBA

    \newpage

    \tableofcontents

    \newpage

    \section{Introduction}

    \subsection{Context}
    Regression analysis have become a corner stone of modern statistical analysis. Discovering the relationship between dependeant and independat variables offers great insight into the underlying system and can provide far reaching predictive capabilities. Linear models describe the relationship explain the dependant variable as the sum of the independant variables and the linear interaction with a coefficent. The general additive model describes a modelling approach which explores the relationship between dependant and independant variables as the sum of smoothing functions. This modeling approach offers more flexiblity and power than linear model and has been succesfuly established in academia and industry. However, the additional power of GAMs comes at the cost of additional computational effort, placing strict constraints on the model and data size. Modern literatur suggest two optimization to eliviate this constraint: parallelization and numerical optimization.
    The optimizations have been implmented and used in high-level statiscial languanges like R and SPASS. While they are already allowing to model fit on more data I feel their implementation in a truly distributed enviroment could bring more power. Apache Spark offers a general purpose cluster computing engine. It's general and distributed nature offers the ideal enviroment to fit general additive models in a distributed enviroment.

    My thesis and this expose is closely tied to \cite{gamBook}. \cite{gamBook} establishes all relevant theory to GAMs and \cite{bigdataGAM} extends this theory to the context of large data sets.


    \section{Content}
    \subsection{General Linear Models}
    Linear models are statistical models in which a univariate response is modelled as the sum of a ‘linear predictor’ and a zero mean random error term. The linear predictor depends on some predictor variables, measured with the response variable, and some unknown parameters, which must be estimated. A key feature of linear models is that the linear predictor depends linearly on these parameters. Statistical inference with such models is usually based on the assumption that the response variable has a normal distribution. Linear models are used widely in most branches of science, both in the analysis of designed experiments. \

    \begin{equation}  \label{linModel} y\textsubscript{i} = \beta x\textsubscript{i} + \varepsilon\textsubscript{i} \end{equation}

    The equation \ref{linModel} introduces formal notation on how to describe the dependant variable y as linear combination of x and the unknown parameter \textbeta plus an error term \textepsilon. We are seeking \textbeta to fit all data as closely as possible and introduce S as the squared sum which serves as a good proxy for goodness of fit.

    \begin{equation} \label{sumSquares} \mathbf{S} =  \sum_{i=1}^{n}(y\textsubscript{i} - x\textsubscript{i}\beta)^{2} \end{equation}

    The common measure of \textbeta is the sum of squares defined in \ref{sumSquares}, Markov-Gaus have proven that the best possible unbiased estimator is \textbeta  the minimization of \ref{sumSquares}.

    \begin{equation}  \label{linMatrixModel} y =  \mathbf{X} \beta \end{equation}

    We can rewrite \ref{linModel} in scalar form \ref{linMatrixModel}. The dependante variable vector \textit{y} is the linear combination of model matrix \textit{X} and the vector of unknown parameter \textbeta.

    \subsection{Generalized Linear Models}
    Generalized linear models (GLMs) somewhat relax the strict linearity assumption of linear models, by allowing the expected value of the response to depend on a smooth monotonic function of the linear predictor. Similarly the assumption that the response is normally distributed is relaxed by allowing it to follow any distribution from the exponential family (for example, normal, Poisson, binomial, gamma etc.).

    \begin{equation} \label{glmMatrix} g(\mu\textsubscript{i}) = \mathbf{X} \beta \end{equation}

    Equation \ref{glmMatrix} introduces formal notation for GLMs and illustrates the simmilarities to the genral linear model. It is important to recognize the smooth monotonic function \textit{g()}. The smooth monotonic function, also known as link function, is the key extension which enables to model members of the expotential family with a linear model.

    \subsection{Generalized Additive Models}
    A Generalized Additive Model (GAM) extends the GLM in by specifing the linear prediction in terms of a sum of smooth functions. The exact parametric form of these functions is unknown, as is the degree of smoothness appropriate for each of them. GAMs extend GLMs with the following mechanics:
    \begin{itemize}
        \item The smooth functions must be represented somehow.
        \item The degree of smoothness of the functions must be made controllable, so that models with varying degrees of smoothness can be explored.
        \item Some means for estimating the most appropriate degree of smoothness from data is required, if the models are to be useful for more than purely exploratory work.
    \end{itemize}
    With the outlined additions to GLMs we can now introduce more notation to get a better unterstanding of GAMs.

    \begin{equation} \label{GAM} g(\mu\textsubscript{i}) = \mathbf{X} \textsubscript{i} \Theta + f\textsubscript{1}(x\textsubscript{1i}) + f\textsubscript{2}(x\textsubscript{2i}) + f\textsubscript{3}(x\textsubscript{3i}, x\textsubscript{4i}) ... \end{equation}

    \ref{GAM} explains the y as a It is important to note that yi = E(Yi). Xi is a row of the model matrix with parametric component. O is the parameter vector and fJ are the smooth function of the covariates xk. Specifing Yi only interms of smooth functions allows for more flexible modelling. The smoothing function f is best understood in a simple univariate case illustrated bellow.

    \begin{equation} \label{univariateSmooth} y_i = f(x_i) + \epsilon_i \end{equation}

    Finding the right smooth function stands at the heart of GAM fitting. One way to find f is to confine it's space to a choosen basis function. To keep the within the theory already developed for linear models we require the smoothing function to be represented by a linear basis. For the sake of illustration we assume \ref{univariateSmooth} can be rewrite as the following equation if bi(x) is the ith basis function:

    \begin{equation} \label{smoothBase} f(x) = \sum_{i=1}^{q} b\textsubscript{i} (x) \beta \textsubscript{i} \end{equation}

    In \ref{smoothBase} we the treat the b to be completly known and the linearity of \ref{smoothBase} is given by \textbeta. To make this a bit more concrete we now assume our basis function to be a fourth order polynomial allowing us to rewrite \ref{smoothBase}

    \begin{equation} \label{poly4th} f(x) = \beta \textsubscript{1} + x\beta \textsubscript{2} + x^2\beta \textsubscript{3} + x^3\beta \textsubscript{4} + x^4\beta \textsubscript{5}  \end{equation}

    Which when applied to \label{univariateSmooth} yields the full model for a univariate GAM.

    \begin{equation} \label{poly4thGAM}  y_i = \beta_1 + x_i\beta_2 + x_i^2\beta_3+ x_i^3\beta_4 + x_i^4\beta_5 + \epsilon_i \end{equation}

    Equation \ref{poly4thGAM} gives a formal full description of  y\textsubscript{i} as the sum of a fourth degree polynomial. Fourth order polynomial basis function enforce rather strict limitations and often struggle to provide a desireable fit. Instead of trying to fit a single fourth order polynomial to all our data points we will now seek to fit a fixed degree polynomials between certain sections of our curve. This approach is called regression splines and follows this general pattern:
    \begin{itemize}

    \item Divide the curve in to a fixed number of sections, the point between two sections is called a knot.
    \item Find a nth degree polynomial for each section between two knot
    \item Each nth degree polynomial must match its neighboors in value, first and second derivative at the location of the knot.
    \item Each spline has zero second derivatives at the knot location to ensure a smooth transition.

    \end{itemize}

    Using fixed degree polynomial splines as a basis for f means that \ref{univariateSmooth} becomes a linear model identical to \ref{linMatrixModel}, this is very exciting because we can now use well established linear model theory to fit the very flexible class of GAMs.

    The number and location of knots as well as the degree of the polynomial are a crucial part of GAMs. One could use hypthesis testing or backward selection to find a good configuration of these paramters but choosing a base function and controlling the smooth by introducing a penalty has been proven very successful in literatur.

    The penalty factor controlls the smoothnes by introducing a parameter \textlambda\ that scales the second derivative of the smooth function f. Fitting GAMs can thus be restated as the minimization of the following equation:

    \begin{equation} \label{losswithpenalty} \left \| y - X\beta  \right \|^2 + \lambda \int_{0}^{1} [{f}''(x)]^2 dx \end{equation}

    The integrated square root of second deriviative penalizes models that have a too 'rough' function. As \textlambda approaches 0 f will become a straight regression line while \textlambda of 0 becomes an un-penelized regression spline. The trade off between model fit and and model smoothness is controlled by the smoothing parameter \textlambda.

    The linear nature of f allows us to rewrite \ref{losswithpenalty} in it's final form:

    \begin{equation} \label{FINALSMOOTH} \left \| y - X\beta  \right \|^2 + \lambda \beta^T S\beta  \end{equation}

    \ref{FINALSMOOTH} shows that describing an unknown variable as the sum of smooth functions can be restated as the minimization of penelized regression splines and hence becomes problem of minimizing equation \ref{FINALSMOOTH}. We have now stated the essential problem of GAM fitting and will now look at options to solve that problem

    \subsection{Smoothing parameter Estimation with generalized cross validation}
    Finding a good smoothing parameter through minimizing \ref{FINALSMOOTH} poses a central question for GAM research, I will now outline the most common algorithm and conclude with a runtime analysis of that method.

     B is any square root of the matrix S such that tr(B) B = S. The sum of squares term, on the right hand side, is just a least squares objective for a model in which the model matrix has been augmented by a square root of the penalty matrix, while the response data vector has been augmented with q zeros. B can be obtained easily by spectral decomposition or pivoted Choleski decomposition and once obtained the augmented least squares problem can be solved using orthogonal methods, in order to solve the penalized least squares problem and fit the model. The only remaining input paramether is \textlambda. Varying \textlambda will yield different fits either close. Figure illustrates different choices for \textlambda and its influence on the fit.

    A high \textlambda will overfit the data while a low \textlambda will create undersmoothed splines. Ideally we want a spline f' that is very similar to f. We can find a optimal \textlambda that influences f' to be as closely to f with the minimization of the average distance between f and f'. A suitable proxy for a choice of a good \textlambda is to minimize M:

    \begin{equation} \label{GCV} M = \frac{1}{n} \sum_{i=1}^{n} (\widehat{f}(x_i) - f(x_i))^2 \end{equation}

    M cannot be used directly, but it is possible to derive an estimate of E(M) + σ2, which is the expected squared error in predicting a new variable. Since fhe function f is unknown we have to find a mechanism to validate our smoothing parameter withour knowing f.

    We will now introduce general cross validation (gcv). GCV seeks to valiade a choice of \textlambda by scoring the fit of M only on data that was not used to fit M. We are validating our model fit across seen and unseen data. This appraoch introduces f'-i which represents the model fitted to alldata except yi. The proposed method fits the model on the estimated dataIf models are judged only by their ability to fit the data from which they were estimated, then complicated models are always selected over simpler ones. Choosing a model in order to maximize the ability to predict data to which the model was not fitted, does not suffer from this problem. GCV is formalized with the following equation:

    \begin{equation} \label{FULLGCV} V_o = \frac{1}{n} \sum_{i=1}^{n} (\widehat{f}_i\textsuperscript{[-i]}-y_i)^2 \end{equation}

    \ref{FULLGCV} describes the process of finding the optimal \textlambda. General cross validation checks a \textlambda

    \begin{equation} V_g(\lambda) = \frac{n\left \| y-X\widehat{\beta}_\lambda \right \| ^2}{\{n-tr(F_\lambda) \}^2} \end{equation}

    The application of the GCV to our problem yields following description of \textlambda

    \begin{equation} \label{GCV2} V_g(\lambda) = \frac{n\left \| y-X\widehat{\beta}_\lambda \right \| ^2}{\{n-tr(F_\lambda) \}^2} \end{equation}

    The main issue with \ref{FULLGCV} is the requirement to form the whole model X at each iteration to find \textlambda. The difficulty is simply that the model matrix for the model can become too big: if n and p are respectively the number of rows and columns of the model matrix, and M is the number of smoothing parameters, then the memory requirements of GAM fitting methods are typically $O(Mnp^2)$. The aim of my bachelor thesis is to lay out a faster way of finding the smoothing term.



    \subsection{Generalized Additive Models for large data sets}
    \cite{bigdataGAM} introduce an extension of the developmed methods for fitting GAMs for very large data sets. The main advantage propossed by Woods is the rewriting the general cross validation step to not require the full model matrix X in each step. Woods describes a  simple strategies for updating a model matrix factorization can be used to avoid formation of the whole model matrix in the GAM context. Most importantly, Woods shows how to adapt smoothing parameter estimation methods in this setting.

    Now suppose that the model matrix is first QR decomposed into a column orthogonal n × p factor Q and an upper triangular p×p factor R so that $X=QR$: If we also form $f =QTy$ and $∥r∥^2 = ∥y∥^2 − ∥f∥^2$ then expression \label{FINALSMOOTH} becomes

    \begin{equation} \label{NEWGCV} \left \| f- R\beta \right \|^2 + \left \| r \right \|^2 + \sum_{j}^{ } \lambda_j \beta^T S_j \beta \end{equation}. \ref{NEWGCV} allows us to rewrite \ref{NEWGCV} in a form that does not require the full model matrix X.

    \begin{equation} \label{NEWNEWGCV} V_g(\lambda) = \frac{n\left \| f-R\widehat{\beta}_\lambda \right \| ^2 + \left  \| r \right \| ^2}{\{n-tr(F_\lambda) \}^2} \end{equation}.

    This transformation from \ref{GCV2} to \ref{NEWNEWGCV} is the corner stone of my bachelor thesis. Finding the ideal \textlambda using the GCV method without forming the full model matrix X allows us eradicate previous model size limitations. A further improvement can be



    \subsection{Apache Spark}
    Distributed computing is a special interest of mine. I've had the pleasure of setting up a SparkR cluster and have been following its development closely. Apache Spark advertises itself as a fast and general cluster computing engine for large-scale data processing. He a overview of it's key features:
    \begin{itemize}
        \item fast, Apache Spark currently holds the world record in sorting 1 TB on data, beating the previus record help by Hadoop. Spark reaches this high speed by removing I/O to disc and maintaining all relevant in memory.
        \item general, Aparche Spark offers a high level API called Resilient Data Set. RDDs are designed to facilitate a resiliend, distributed data set as a optimal level of interaction with data. Each of the key features of RDDs are outlined below:
        \begin{itemize}
            \item Resilient: Every RDD is evaluated lazy and constructed from it's lineage, point to different previous RDD.
            \item Distributed: RDDs are partiioned accross many machines
            \item Data set: RDDs are ment to think about in terms of matrixes, this should feel very natural for people familar with data.
        \end{itemize}
        \item cluster computing, Apache Spark is build to be executed in a cluster enviroment like Mesos or Yarn. The management and design of these systems are not subject of my bachelor thesis but are centainly worthy of one.
        \item large scale data processing, the data managable by Apache Spark is limited by the amount of memory availble in a cluster. The advent of obiquis cluster computing and the ability to scale horizontaly
    \end{itemize} The implementation of the outlined optimization in the previously described enviroment is explained in the next section.

    \subsection{General additive models in Apache Spark}
    A general
    \begin{itemize}
        \item QR Update
        \item Parallel computation
        \item Getting f, r and bla without X
        \begin{itemize}
            \item F
            \item R
            \item X
        \end{itemize}
    \end{itemize}

    \section{Guiding Questions}

    \paragraph{Can the distributed, numerically optimized, in-memory version of GAM fitting beat previous benchmark implementation in model size, data size and fitting speed?}

    \paragraph{}
    Subquestions arising are:
        \begin{itemize}
        \item How much faster is a parallel variant against the sequential one?
        \item How much faster is the Spark in-memory variant against the R one?
        \item What is the new limit for data and model size?
        \end{itemize}

    \subsection{Specific Approaches in Apache Spark}
    The following is a brief comparision of the candidate approaches for my thesis.




    \subsection{Deciding on an approach}

    I have decided to implement the fitting of GAMs in Apache Spark. My choice will be explained now:
    \paragraph{Nested Data Parallel}
        Spark is fast
        Iterative nature is well suited for in-memory



    \subsection{What is novel in this approach?}
    The suggested approach has been implemented in high-level statistical languages, proving its effectiveness. The suggested methods have not been implemented in an enviroment designed for in-memory cluster computing. As I lined out earlier the iterative nature of the proposed methods is well suited for a distributed in-memory enviroment and hence should be discoved

    \subsection{When is the thesis regarded as completed?}
    The thesis is considered finished, once various strategies of
    implementing a chosen algorithm (e.g.  have been
    implemented and compared in effectiveness/efficiency/human-workload.

    \subsection{Which tasks have to be completed for this thesis?}
    My work is defined by creating the progracomparing them.
    The focus is on showing the various transformations applied in NDP, which the programmer would have had done manually.













    \paragraph{Additional tasks}
        \begin{itemize}
            \item Deeping theoretical unterstanding of GAMs
            \item Read futher literature on how to make quantified comparisions of the programs (e.g. parallel complexity, benchmarks, etc...)
            \item Decide whether I will work with the true Haskell-Core code generated in    or apply the flattening and fusioning transformations manually.
                The first variant is the actual source code that will later be executed - but it is hard to read. \footnote[1]{See \texttt{DotP.hs} and \texttt{DotP.vect.hs} at \href{https://github.com/GollyTicker/Nested-Data-Parallel-Haskell/tree/0e8d3df0d8084a01b007b27debda2b64247a254d}{my github repo}. }
                The implementation of DPH is less developed than its theoretical background.
                The second variant is easier to work with and simpler to present, however it will almost certainly be less optimized than the actual code.
            \item Decide which algorithm(s) is/are to be used as examples. They should be easily visualizable and should have irregular behaviour.
                Connected-Components-Labeling on images is such an candidate. Combining multiple algorithms into
                a pipeline is also attractive, since cross-algorithm fusionsing and optimization is where Haskell can shine truely.
            \item Learn how make benchmarks/create runtime statistics
        \end{itemize}

    \subsection{Which tasks are realistic to be accomplished within the scope of this thesis?}
    I am optimistic that these task can be accomplished in a timeframe of 2 months. However, I might need to keep a tight focus or
    choose a simple (and small) algorithm to keep the task accomplishable.

    \section{Time table}
    My time plan is visible at table \ref{timetable}.
    \begin{table}[h]
        \begin{center}
        \caption{Time table} % http://www.latex-tutorial.com/tutorials/beginners/lesson-8/
        \label{timetable}
        \begin{tabular}{rrl}
            \toprule
            CW & monday & thesis work \\
            \midrule
            17 & 20.04 & reading remaining papers, reading parallel complexity theory \\
            18 & 27.04 & deciding on an algorithm, learning benchmarking  \\
            19 & 4.05  & impleme \\
            20 & 11.05  asd \\
            21 & 18.05 & vectori \\
            22 & 25.05 asd \\
            23 & 1.06  & \textit{puffer} \\
            24 & 8.06  & \textit{puffer} \\
            25 & 15.06 & Begin to write down, prepare for exams\\
            26 & 22.06 & Writing..., prepare for exams \\
            27 & 29.06 & Writing..., prepare for exams \\
            28 & 6.07  & Prepare Colloquium, Writing..., exams week 1 \\
            29 & 13.07 & Prepare Colloquium, Finalize writing, exams week 1 \\
            30 & 20.07 & Colloquium and Release \\
            31 & 27.07 & Last week for Colloquium and Release \\
            32 & 3.08  & Fin \texttt{:D} \\
        \end{tabular}
        \end{center}
    \end{table}


    \newpage

    \bibliography{expose}    % reference to expose.bib

    \newpage

\end{document}
